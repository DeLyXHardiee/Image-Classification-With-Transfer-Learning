{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning vs. Training From Scratch\n",
    "\n",
    "This notebook compares the performance of:\n",
    "1. **Transfer Learning**: Using pre-trained ResNet50 weights\n",
    "2. **Training From Scratch**: Randomly initialized ResNet50\n",
    "\n",
    "We'll evaluate both approaches on the same dataset to understand the benefits of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 15,  # More epochs to see convergence difference\n",
    "    'learning_rate': 0.001,\n",
    "    'num_classes': 5,\n",
    "    'train_split': 0.8,\n",
    "    'image_size': 224,\n",
    "    'num_workers': 2,\n",
    "}\n",
    "\n",
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    full_dataset = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    class_names = full_dataset.classes\n",
    "    print(f\"Loaded custom dataset with {len(full_dataset)} images\")\n",
    "except:\n",
    "    print(\"Using Flowers102 for demonstration...\")\n",
    "    full_dataset = datasets.Flowers102(root='./data', split='train', download=True, transform=train_transforms)\n",
    "    num_classes = 102\n",
    "    class_names = [f'class_{i}' for i in range(num_classes)]\n",
    "\n",
    "CONFIG['num_classes'] = num_classes\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(CONFIG['train_split'] * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "                         num_workers=CONFIG['num_workers'], pin_memory=True if torch.cuda.is_available() else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                       num_workers=CONFIG['num_workers'], pin_memory=True if torch.cuda.is_available() else False)\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} | Val set: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc='Training', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100 * correct / total\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Validating', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100 * correct / total\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name):\n",
    "    \"\"\"Train model and return history.\"\"\"\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'epoch_times': []}\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\\n\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    history['total_time'] = total_time\n",
    "    history['best_val_acc'] = best_val_acc\n",
    "    \n",
    "    print(f\"\\nTotal training time: {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\\n\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 1: Transfer Learning (Pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50\n",
    "model_transfer = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer\n",
    "num_features = model_transfer.fc.in_features\n",
    "model_transfer.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, CONFIG['num_classes'])\n",
    ")\n",
    "\n",
    "model_transfer = model_transfer.to(device)\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_transfer.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_transfer.parameters())\n",
    "\n",
    "print(f\"Transfer Learning Model:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train transfer learning model\n",
    "history_transfer = train_model(\n",
    "    model_transfer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion_transfer,\n",
    "    optimizer_transfer,\n",
    "    CONFIG['num_epochs'],\n",
    "    device,\n",
    "    \"Transfer Learning (Pre-trained ResNet50)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 2: Training From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ResNet50 with random weights\n",
    "model_scratch = models.resnet50(weights=None)  # No pre-trained weights\n",
    "\n",
    "# Replace final layer\n",
    "num_features = model_scratch.fc.in_features\n",
    "model_scratch.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, CONFIG['num_classes'])\n",
    ")\n",
    "\n",
    "model_scratch = model_scratch.to(device)\n",
    "\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "# Train all parameters from scratch\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "trainable_params_scratch = sum(p.numel() for p in model_scratch.parameters() if p.requires_grad)\n",
    "total_params_scratch = sum(p.numel() for p in model_scratch.parameters())\n",
    "\n",
    "print(f\"From-Scratch Model:\")\n",
    "print(f\"  Total parameters: {total_params_scratch:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params_scratch:,} ({100*trainable_params_scratch/total_params_scratch:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from-scratch model\n",
    "history_scratch = train_model(\n",
    "    model_scratch,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion_scratch,\n",
    "    optimizer_scratch,\n",
    "    CONFIG['num_epochs'],\n",
    "    device,\n",
    "    \"Training From Scratch (Random Init ResNet50)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "epochs = range(1, CONFIG['num_epochs'] + 1)\n",
    "\n",
    "# Training Loss\n",
    "axes[0, 0].plot(epochs, history_transfer['train_loss'], 'b-o', label='Transfer Learning', linewidth=2)\n",
    "axes[0, 0].plot(epochs, history_scratch['train_loss'], 'r-s', label='From Scratch', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "axes[0, 1].plot(epochs, history_transfer['val_loss'], 'b-o', label='Transfer Learning', linewidth=2)\n",
    "axes[0, 1].plot(epochs, history_scratch['val_loss'], 'r-s', label='From Scratch', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy\n",
    "axes[1, 0].plot(epochs, history_transfer['train_acc'], 'b-o', label='Transfer Learning', linewidth=2)\n",
    "axes[1, 0].plot(epochs, history_scratch['train_acc'], 'r-s', label='From Scratch', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "axes[1, 1].plot(epochs, history_transfer['val_acc'], 'b-o', label='Transfer Learning', linewidth=2)\n",
    "axes[1, 1].plot(epochs, history_scratch['val_acc'], 'r-s', label='From Scratch', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('transfer_vs_scratch_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Best Validation Accuracy (%)',\n",
    "        'Final Validation Accuracy (%)',\n",
    "        'Final Training Accuracy (%)',\n",
    "        'Total Training Time (min)',\n",
    "        'Avg Time per Epoch (s)',\n",
    "        'Trainable Parameters',\n",
    "        'Convergence Speed'\n",
    "    ],\n",
    "    'Transfer Learning': [\n",
    "        f\"{history_transfer['best_val_acc']:.2f}\",\n",
    "        f\"{history_transfer['val_acc'][-1]:.2f}\",\n",
    "        f\"{history_transfer['train_acc'][-1]:.2f}\",\n",
    "        f\"{history_transfer['total_time']/60:.2f}\",\n",
    "        f\"{np.mean(history_transfer['epoch_times']):.2f}\",\n",
    "        f\"{trainable_params:,}\",\n",
    "        'Fast'\n",
    "    ],\n",
    "    'From Scratch': [\n",
    "        f\"{history_scratch['best_val_acc']:.2f}\",\n",
    "        f\"{history_scratch['val_acc'][-1]:.2f}\",\n",
    "        f\"{history_scratch['train_acc'][-1]:.2f}\",\n",
    "        f\"{history_scratch['total_time']/60:.2f}\",\n",
    "        f\"{np.mean(history_scratch['epoch_times']):.2f}\",\n",
    "        f\"{trainable_params_scratch:,}\",\n",
    "        'Slow'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON: TRANSFER LEARNING VS. TRAINING FROM SCRATCH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<35} {'Transfer Learning':<20} {'From Scratch':<20}\")\n",
    "print(\"-\"*80)\n",
    "for i, metric in enumerate(comparison_data['Metric']):\n",
    "    print(f\"{metric:<35} {comparison_data['Transfer Learning'][i]:<20} {comparison_data['From Scratch'][i]:<20}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bar Chart Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "metrics = ['Best Val Acc', 'Final Val Acc', 'Final Train Acc']\n",
    "transfer_values = [\n",
    "    history_transfer['best_val_acc'],\n",
    "    history_transfer['val_acc'][-1],\n",
    "    history_transfer['train_acc'][-1]\n",
    "]\n",
    "scratch_values = [\n",
    "    history_scratch['best_val_acc'],\n",
    "    history_scratch['val_acc'][-1],\n",
    "    history_scratch['train_acc'][-1]\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, transfer_values, width, label='Transfer Learning', color='steelblue')\n",
    "axes[0].bar(x + width/2, scratch_values, width, label='From Scratch', color='coral')\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics, rotation=15, ha='right')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Training time comparison\n",
    "time_metrics = ['Total Time (min)', 'Avg Epoch Time (s)']\n",
    "transfer_times = [\n",
    "    history_transfer['total_time']/60,\n",
    "    np.mean(history_transfer['epoch_times'])\n",
    "]\n",
    "scratch_times = [\n",
    "    history_scratch['total_time']/60,\n",
    "    np.mean(history_scratch['epoch_times'])\n",
    "]\n",
    "\n",
    "x2 = np.arange(len(time_metrics))\n",
    "axes[1].bar(x2 - width/2, transfer_times, width, label='Transfer Learning', color='steelblue')\n",
    "axes[1].bar(x2 + width/2, scratch_times, width, label='From Scratch', color='coral')\n",
    "axes[1].set_ylabel('Time', fontsize=12)\n",
    "axes[1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x2)\n",
    "axes[1].set_xticklabels(time_metrics)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_bar_charts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Learning Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement rate\n",
    "def calculate_improvement(history, start_epoch=0, end_epoch=5):\n",
    "    \"\"\"Calculate accuracy improvement rate.\"\"\"\n",
    "    start_acc = history['val_acc'][start_epoch]\n",
    "    end_acc = history['val_acc'][min(end_epoch, len(history['val_acc'])-1)]\n",
    "    return end_acc - start_acc\n",
    "\n",
    "transfer_early_improvement = calculate_improvement(history_transfer, 0, 5)\n",
    "scratch_early_improvement = calculate_improvement(history_scratch, 0, 5)\n",
    "\n",
    "print(\"\\nLearning Speed Analysis (First 5 epochs):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Transfer Learning improvement: {transfer_early_improvement:.2f}%\")\n",
    "print(f\"From Scratch improvement: {scratch_early_improvement:.2f}%\")\n",
    "print(f\"\\nTransfer learning converges {transfer_early_improvement/scratch_early_improvement:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS: TRANSFER LEARNING VS. TRAINING FROM SCRATCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. ACCURACY:\")\n",
    "acc_diff = history_transfer['best_val_acc'] - history_scratch['best_val_acc']\n",
    "if acc_diff > 0:\n",
    "    print(f\"   ✓ Transfer learning achieved {acc_diff:.2f}% higher accuracy\")\n",
    "else:\n",
    "    print(f\"   ⚠ From-scratch training achieved {abs(acc_diff):.2f}% higher accuracy\")\n",
    "\n",
    "print(\"\\n2. CONVERGENCE SPEED:\")\n",
    "print(f\"   ✓ Transfer learning: {transfer_early_improvement:.2f}% improvement in 5 epochs\")\n",
    "print(f\"   ✓ From scratch: {scratch_early_improvement:.2f}% improvement in 5 epochs\")\n",
    "print(f\"   → Transfer learning converges ~{transfer_early_improvement/max(scratch_early_improvement, 0.1):.1f}x faster\")\n",
    "\n",
    "print(\"\\n3. COMPUTATIONAL EFFICIENCY:\")\n",
    "time_ratio = history_scratch['total_time'] / history_transfer['total_time']\n",
    "print(f\"   ✓ Transfer learning trains {trainable_params/trainable_params_scratch*100:.1f}% fewer parameters\")\n",
    "if time_ratio > 1:\n",
    "    print(f\"   ✓ Transfer learning is {time_ratio:.2f}x faster per epoch\")\n",
    "else:\n",
    "    print(f\"   ⚠ From-scratch is {1/time_ratio:.2f}x faster per epoch (fewer frozen layers)\")\n",
    "\n",
    "print(\"\\n4. OVERFITTING:\")\n",
    "transfer_gap = history_transfer['train_acc'][-1] - history_transfer['val_acc'][-1]\n",
    "scratch_gap = history_scratch['train_acc'][-1] - history_scratch['val_acc'][-1]\n",
    "print(f\"   ✓ Transfer learning train-val gap: {transfer_gap:.2f}%\")\n",
    "print(f\"   ✓ From scratch train-val gap: {scratch_gap:.2f}%\")\n",
    "if transfer_gap < scratch_gap:\n",
    "    print(f\"   → Transfer learning shows better generalization\")\n",
    "else:\n",
    "    print(f\"   → From-scratch shows better generalization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Use TRANSFER LEARNING when:\")\n",
    "print(\"  • Working with small datasets (< 10k images)\")\n",
    "print(\"  • Limited computational resources\")\n",
    "print(\"  • Quick prototyping needed\")\n",
    "print(\"  • Task is similar to ImageNet (natural images)\")\n",
    "print(\"\\n✓ Train FROM SCRATCH when:\")\n",
    "print(\"  • Large dataset available (> 100k images)\")\n",
    "print(\"  • Domain is very different from ImageNet (medical, satellite, etc.)\")\n",
    "print(\"  • Sufficient computational resources\")\n",
    "print(\"  • Need complete control over feature learning\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comparison demonstrates the significant advantages of transfer learning:\n",
    "\n",
    "### Transfer Learning Benefits:\n",
    "1. **Faster convergence**: Achieves good performance in fewer epochs\n",
    "2. **Better accuracy**: Leverages pre-trained features from ImageNet\n",
    "3. **Less overfitting**: Pre-trained features provide good regularization\n",
    "4. **Efficient training**: Only trains final layers, reducing computation\n",
    "5. **Small data friendly**: Works well with limited training samples\n",
    "\n",
    "### When Transfer Learning Works Best:\n",
    "- Small to medium datasets (≤ 10k images)\n",
    "- Similar domain to pre-training dataset (natural images)\n",
    "- Limited computational resources\n",
    "- Time-constrained projects\n",
    "\n",
    "### Training From Scratch:\n",
    "- May eventually match or exceed transfer learning with enough data and epochs\n",
    "- Better for domains very different from ImageNet\n",
    "- Requires significantly more training time and data\n",
    "- Higher risk of overfitting on small datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
